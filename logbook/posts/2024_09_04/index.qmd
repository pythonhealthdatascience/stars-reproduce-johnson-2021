---
title: "Day 5"
author: "Amy Heather"
date: "2024-09-04"
categories: [reproduce]
bibliography: ../../../quarto_site/references.bib
---

::: {.callout-note}

Fixed my loading of epicR so get model results! Experiemented with agent numbers. Total time used: -h -m (-%)

:::

## 09.19-09.41: Returning to run the model

Did a quick review of my prior logbook entries to get back up to speed. Will be trying to run `Mini_Analysis.Rmd`, which I had created previously and contains only scenario S1NoCD and saves the result to csv (whilst the full script `Case_Detection_Results.Rmd` contains many more scenarios. I had previously reduced the number of agents (`settings$n_base_agents <- `) to try and run on my local machine but this just returned 0/1/NaN, so I will now be trying with the full number of agents.

In `Case_Detection_Results.Rmd`, it sets `settings$n_base_agents <- 50e+6` but has comment "change number of agents here: 100e+6 is used in the analysis".

I decided to start with trying `Mini_Analysis.Rmd` with 50e+6. I logged into the remote computer, cloned the GitHub repository, then ran the following:

```
cd stars-reproduce-johnson-2021/reproduction
Rscript -e "renv::restore()"
Rscript -e "rmarkdown::render('scripts/Mini_Analysis.Rmd')"
```

This however returned an error that pandoc was required, so I ran `sudo apt install pandoc` then tried again. This then successfully rendered the `.Rmd`. I initially did 5000 agents to make sure it worked and output correctly, and then the original 50e+6 as mentioned.

## 11.29-11.34, 11.48-12.00: Reviewing results from 50e+6

This ran in 1.642896 hours (1 hour 38 minutes). The result was still wrong though:

```{python}
import pandas as pd
pd.read_csv('mini_analysis_50e6.csv')
```

From the Case_Retection_Results.md in their repository, I can see that I should be getting numbers like 37195539 agents, 625946560 patient years, 71206641 CopdPYs, and so on.

I'm sceptical that increasing to 100e+6 would fix that issue, as their notebooks appear to have been run with 50e+6, as I did.

I double-checked the code in Mini_Analysis.md, but was satisfied that this matched up with theirs, with the only changes being to import epicR from the local folder, and to add a timer, and not having some processing of the table (but that would make no difference here, as numbers 0 and NaN).

I'm presuming the issue therefore is in the `epicR` package I have, but I'm rather unsure on where to start, or where the issue might be!

## 12.01-12.24: Basic run of epicR to resolve the "0" results issue

As I starting point, I tried following the instructions available for running epicR generally (and not this specific study), to see if I could get it working and printing some results for a random basic scenario.

On <https://github.com/resplab/epicR>, they give code for running epicR with default settings:

```
library(epicR)
init_session()
run()
Cget_output()
terminate_session()
```

And the default inputs viewed with:

```
input <- get_input()
```

However, I later realised this was a newer function in epicR and not in the version we have.

I loaded my local epicR package, although without warning=False, realised it had a warning:

```
Warning: ── Conflicts ───────────────────────────────────────────────────────────────────────────────── epicR conflicts
──
✖ `Cget_agent_events` masks `epicR::Cget_agent_events()`.
✖ `Cget_all_events` masks `epicR::Cget_all_events()`.
✖ `Cget_all_events_matrix` masks `epicR::Cget_all_events_matrix()`.
  … and more.
ℹ Did you accidentally source a file rather than using `load_all()`?
  Run `rm(list = c("Cget_agent_events", "Cget_all_events", "Cget_all_events_matrix", "Cget_event",
  "Cget_events_by_type", "Cget_inputs", "Cget_n_events", "Cget_output", "Cget_output_ex", "Cget_runtime_stats",
  "Cget_settings", "Cget_smith", "Cset_input_var", "Cset_settings_var", "get_sample_output", "mvrnormArma"))` to
  remove the conflicts.
```

I found that, if I ran the whole script just with the local `devtools::load_all("../epicR")`, it would appear to run but give results like 0. If however I cleared the environment and then ran `library(epicR)` below the `load_all()` statement, it would run and give actual results. I'm presuming this relates to the conflicts message above. I understand that, when I clear the environment, I'm removing objects, states and data that might have come with load_all() but keeping the package itself, so I am just left with a "clean slate" with the package.

## 12.25-12.30, 14.08-14.53, 15.10-15.14, 15.20-15.25: Fixing my mini analysis and returning to lower numbers of agents

I removed the `Basic_Model.Rmd` and returned to `Mini_Analysis.Rmd`, adding the new workflow of clearing the environment and reloading the library. I returned to running it with fewer numbers of agents. This output results - hurrah!

As I'd tried before, ran these (either on local machine or more high powered remote machine):

* 5000 (5e3)
* 50,000 (5e4) (17 seconds local, 7.2 seconds remote)
* 500,000 (5e5) (2.6 minutes local, 1.0 minutes remote)
* 1 million (1e6) (4.7 minutes local, 2.0 minutes remote)
* 5 million (5e6) (27.3 minutes local, 10.0 minutes remote)

And it was quicker than before too! (17s vs 2.4m, 2.6m vs 18.2m).

```{python}
import pandas as pd

# Import results
df5e3 = pd.read_csv('mini_analysis_fixed_5e3.csv').rename(index={0: '5e3'})

df5e4 = pd.read_csv('mini_analysis_fixed_5e4.csv').rename(index={0: '5e4'})

df5e5 = pd.read_csv('mini_analysis_fixed_5e5.csv').rename(index={0: '5e5'})

df1e6 = pd.read_csv('mini_analysis_fixed_1e6.csv').rename(index={0: '1e6'})

df5e6 = pd.read_csv('mini_analysis_fixed_5e6.csv').rename(index={0: '5e6'})

# Combine into a single dataframe and show
df_s1nocd = pd.concat([df5e3, df5e4, df5e5, df1e6, df5e6])
df_s1nocd
```

Can't calculate ICER as need incremental costs and qalys, and those need S1a, S1b and S1c to make incrementalcosts, incrementalqaly and incrementalnmb, since they compare against S1NoCD.

Comparing against S1NoCD from `Case_Detection_Results.md`:

* Some columns are different as their numbers depend on number of agents:
  * Agents
  * PatientYears
  * CopdPYs
  * NCaseDetections
  * DiagnosedPYs
  * OverdiagnosedPYs
  * Mild, Moderate, Severe, VerySevere
  * NoCOPD
  * GOLD1, GOLD2, GOLD3, GOLD4
  * Cost
  * QALY
* But some are clearly processed and so should look at these to see if same results:
  * SABA
  * LAMA
  * LAMALABA
  * ICSLAMALABA
  * MildPY, ModeratePY, SeverePY, VerySeverePY
  * CostpAgent
  * QALYpAgent
  * NMB

I filtered to those columns, and add the results from `Case_Detection_Results.md`...

```{python}
# Filter to columns not influenced by agent number
s1 = df_s1nocd[[
  'SABA', 'LAMA', 'LAMALABA', 'ICSLAMALABA', 'MildPY','ModeratePY', 'SeverePY',
  'VerySeverePY', 'CostpAgent', 'QALYpAgent', 'NMB']]

# Add original results to dataframe
original = {
  'SABA': 0.017,
  'LAMA': 0.135,
  'LAMALABA': 0.151,
  'ICSLAMALABA': 0.080,
  'MildPY': 0.217,
  'ModeratePY': 0.041,
  'SeverePY': 0.068,
  'VerySeverePY': 0.006,
  'CostpAgent': 2150.057,
  'QALYpAgent': 12.544,
  'NMB': 625073.2
}
original_row = pd.DataFrame(original, index=['original'])
s1 = pd.concat([s1, original_row])

# View dataframe, rounding to 3dp (as they present it in original)
round(s1, 3)
```

We can see that many columns look very similar between them. However, for MildPY, CostpAgent, QALYpAgent and NMB, the result 5e5 is markedly better than 5e4.

I then add 1 million (1e6) and 5 million (5e6) (already done above). And this looked pretty similar to 500,000 (5e5).

Looking at `Case_Detection_Results.Rmd`, there are 12 scenarios, and then another 12 for 5 years, so a total of 24.

I figured I could try running the whole thing with 500,000 to begin with, but up it to 1 million or 5 million if needed, but that 50 million probably wasn't necessary (as anticipate will likely be able to get similar enough results with lower numbers).

## 15.26-15.38, 15.58-16.08, 16.18-X: Running the full original script

I re-copied the original script and only made the following amendments before running:

* Altering import of epicR
* Adding a timer
* Reducing agent number from **50e6** (50 million) to **5e5** (500 thousand)
* Saving results tables to csv files

Then ran on remote machine. It took a bit of tweaking to correct the saving into outputs folder and get right paths, as I got the path wrong, and then it wouldn't work before had made "outputs" folder.

Finishing this, I got five results tables (saved to csv).

```{python}
pd.read_csv('sall_5e5.csv').sort_values(by='Scenario')
```

```{python}
pd.read_csv('ceplane_5e5.csv')
```

```{python}
pd.read_csv('clinicalresults_5e5.csv')
```

![CEplot](CEplot-1_5e5.png)

## Timings

```{python}
import sys
sys.path.append('../')
from timings import calculate_times

# Minutes used prior to today
used_to_date = 193

# Times from today
times = [
    ('09.19', '09.41'),
    ('11.29', '11.34'),
    ('11.48', '12.00'),
    ('12.01', '12.24'),
    ('12.25', '12.30'),
    ('14.08', '14.53')]
#TODO: Finish adding times, from 15.10 onwards

calculate_times(used_to_date, times)
```